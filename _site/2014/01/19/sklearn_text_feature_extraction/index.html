
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="ujianVerification" content="fbba4ee9b28ec33f11f17698ddc55b92" />
    <link rel="stylesheet" href="/pygments.css">
    <title>sklearn文本特征提取</title>
    
    <meta name="author" content="Cloga Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="/assets/themes/bootstrap/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  
    <!--[if lt IE 9]>
      <script src="/assets/themes/bootstrap/resources/respond/Respond.min.js"></script>
    <![endif]-->

    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
    <script>
      dataLayer = [];
    </script>
  </head>

  <body>
    <nav class="navbar navbar-default" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">Cloga的互联网笔记</a>
        </div>

        <div class="collapse navbar-collapse navbar-ex1-collapse">
          <ul class="nav navbar-nav">
            
            
            


  
    
      
    
  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/pages.html">Pages</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  



          <li><a href="/about.html">关于Cloga</a></li>
          </ul>
        </div>
      </div>
    </nav>
    <div class="container">
      
<div class="page-header">
  <h1>sklearn文本特征提取 </h1>
</div>
<div class="row post-full">
  <div class="col-md-12">
    <div class="date">
      <span>19 January 2014</span>
    </div>
    <div class="content">
      <h1>文本特征提取</h1>

<h2>词袋（Bag of Words）表征</h2>

<p>文本分析是机器学习算法的主要应用领域。但是，文本分析的原始数据无法直接丢给算法，这些原始数据是一组符号，因为大多数算法期望的输入是固定长度的数值特征向量而不是不同长度的文本文件。为了解决这个问题，<a href="http://scikit-learn.org/">scikit-learn</a>提供了一些实用工具可以用最常见的方式从文本内容中抽取数值特征，比如说：</p>

<ul>
<li><em>标记（tokenizing）</em>文本以及为每一个可能的标记（token）分配的一个整型ID ，例如用白空格和标点符号作为标记的分割符（中文的话涉及到分词的问题）</li>
<li><em>计数（counting）</em>标记在每个文本中的出现频率</li>
<li><em>正态化(nomalizating)</em> 降低在大多数样本/文档中都出现的标记的权重</li>
</ul>

<p>在这个方案中，特征和样本的定义如下：</p>

<p>将<em>每个标记出现的频率</em>(无论是否正态化)作为<em>特征</em>。</p>

<p>给定<em>文件</em>中所有标记的出现频率所构成的向量作为多元<em>样本</em>。</p>

<p>因此，语料文件可以用一个词文档矩阵代表，每行是一个文档，每列是一个标记（即词）。</p>

<p>将文档文件转化为数值特征的一般过程被称为<em>向量化</em>。这个特殊的策略（标记，计数和正态化）被称为<em>词袋</em>或者Bag of n-grams表征。用词频描述文档，但是完全忽略词在文档中出现的相对位置信息。</p>

<h2>稀疏性</h2>

<p>大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。</p>

<p>例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。</p>

<p>为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。</p>

<h2>通用向量使用</h2>

<p><em>CountVectorizer</em>在一个类中实现了标记和计数：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</code></pre></div>
<p>这个模型有许多参数，不过默认值已经非常合理（具体细节请见<a href="http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref">参考文档</a>）：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vectorizer</span>

<span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=...</span><span class="s">&#39;word&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">charset</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">charset_error</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">decode_error</span><span class="o">=...</span><span class="s">&#39;strict&#39;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=&lt;...</span> <span class="s">&#39;numpy.int64&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=...</span><span class="s">&#39;utf-8&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=...</span><span class="s">&#39;content&#39;</span><span class="p">,</span>
        <span class="n">lowercase</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">preprocessor</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">strip_accents</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=...</span><span class="s">&#39;(?u)</span><span class="se">\\</span><span class="s">b</span><span class="se">\\</span><span class="s">w</span><span class="se">\\</span><span class="s">w+</span><span class="se">\\</span><span class="s">b&#39;</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">vocabulary</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div>
<p>让我们用它来标记和计算一个简单语料的词频：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
     <span class="s">&#39;This is the first document.&#39;</span><span class="p">,</span>
     <span class="s">&#39;This is the second second document.&#39;</span><span class="p">,</span>
     <span class="s">&#39;And the third one.&#39;</span><span class="p">,</span>
     <span class="s">&#39;Is this the first document?&#39;</span><span class="p">,</span>
 <span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">X</span>                              

<span class="o">&lt;</span><span class="mi">4</span><span class="n">x9</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">int64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">19</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Column</span> <span class="n">format</span><span class="o">&gt;</span>
</code></pre></div>
<p>默认设置通过抽取2个字符以上的词标记字符。完成这个步骤的具体函数可以直接调用：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">analyze</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="n">analyze</span><span class="p">(</span><span class="s">&quot;This is a text document to analyze.&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39;this&#39;</span><span class="p">,</span> <span class="s">&#39;is&#39;</span><span class="p">,</span> <span class="s">&#39;text&#39;</span><span class="p">,</span> <span class="s">&#39;document&#39;</span><span class="p">,</span> <span class="s">&#39;to&#39;</span><span class="p">,</span> <span class="s">&#39;analyze&#39;</span><span class="p">])</span>

<span class="bp">True</span>
</code></pre></div>
<p>在拟合过程中，每一个分析器找到的词都会分配一个在结果矩阵中对应列的整型索引。列的含义可以用下面的方式获得：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39;and&#39;</span><span class="p">,</span> <span class="s">&#39;document&#39;</span><span class="p">,</span> <span class="s">&#39;first&#39;</span><span class="p">,</span> <span class="s">&#39;is&#39;</span><span class="p">,</span> <span class="s">&#39;one&#39;</span><span class="p">,</span>
      <span class="s">&#39;second&#39;</span><span class="p">,</span> <span class="s">&#39;the&#39;</span><span class="p">,</span> <span class="s">&#39;third&#39;</span><span class="p">,</span> <span class="s">&#39;this&#39;</span><span class="p">])</span>

<span class="bp">True</span>

<span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>特征名称与列索引的转化映射被存储在向量器（vectorizer）的vocabulary_属性中：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;document&#39;</span><span class="p">)</span>

<span class="mi">1</span>
</code></pre></div>
<p>因此，在训练语料中没有出现的词在后续调用转化方法时将被完全忽略：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s">&#39;Something completely new.&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>注意在前面的语料中，第一个和最后一个文档的词完全相同因此被编码为等价的向量。但是，我们丢失了最后一个文档是疑问形式的信息。为了保留一些局部顺序信息，我们可以在抽取词的1-grams（词本身）之外，再抽取2-grams：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                     <span class="n">token_pattern</span><span class="o">=</span><span class="s">r&#39;\b\w+\b&#39;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">analyze</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="n">analyze</span><span class="p">(</span><span class="s">&#39;Bi-grams are cool!&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39;bi&#39;</span><span class="p">,</span> <span class="s">&#39;grams&#39;</span><span class="p">,</span> <span class="s">&#39;are&#39;</span><span class="p">,</span> <span class="s">&#39;cool&#39;</span><span class="p">,</span> <span class="s">&#39;bi grams&#39;</span><span class="p">,</span> <span class="s">&#39;grams are&#39;</span><span class="p">,</span> <span class="s">&#39;are cool&#39;</span><span class="p">])</span>

<span class="bp">True</span>
</code></pre></div>
<p>因此，由这个向量器抽取的词表非常大，现在可以解决由于局部位置模型编码的歧义问题：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X_2</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">X_2</span>
<span class="o">...</span>                           
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>
<p>特别是疑问形式“Is this”只出现在最后一个文档：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">feature_index</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;is this&#39;</span><span class="p">)</span>
<span class="n">X_2</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span>     

<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>
<h2>Tf-idf词权重</h2>

<p>在较低的文本语料库中，一些词非常常见（例如，英文中的“the”，“a”，“is”），因此很少带有文档实际内容的有用信息。如果我们将单纯的计数数据直接喂给分类器，那些频繁出现的词会掩盖那些很少出现但是更有意义的词的频率。</p>

<p>为了重新计算特征的计数权重，以便转化为适合分类器使用的浮点值，通常都会进行tf-idf转换。</p>

<p>Tf代表<em>词频</em>，而tf-idf代表词频乘以<em>逆向文档频率</em>。这是一个最初为信息检索（作为搜索引擎结果的排序功能）开发的词加权机制，在文档分类和聚类中也是非常有用的。</p>

<p><em>text.TfidfTransformer</em>类实现了这种正态化：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="n">transformer</span>

<span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=...</span><span class="s">&#39;l2&#39;</span><span class="p">,</span> <span class="n">smooth_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sublinear_tf</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">use_idf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>
<p>同样对于每个参数的详细解释，请参见<a href="http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref">参考文档</a>。</p>

<p>让我们用下面的计数作为例子。第一个词出现每次100%出现因此不是携带的信息不多。另外两个特征只在不到50%的时间出现，因此，对文档内容的代表能力可能更强一些：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">counts</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
           <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
           <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
           <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
           <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
           <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">tfidf</span>                         

<span class="o">&lt;</span><span class="mi">6</span><span class="n">x3</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">9</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>

<span class="n">tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>                        

<span class="n">array</span><span class="p">([[</span> <span class="mf">0.85</span><span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.52</span><span class="o">...</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.55</span><span class="o">...</span><span class="p">,</span>  <span class="mf">0.83</span><span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.63</span><span class="o">...</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">0.77</span><span class="o">...</span><span class="p">]])</span>
</code></pre></div>
<p>每一行被正态化为单位的欧几里得范数。由fit方法计算的每个特征的权重存储在model属性中：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">transformer</span><span class="o">.</span><span class="n">idf_</span>                       

<span class="n">array</span><span class="p">([</span> <span class="mf">1.</span> <span class="o">...</span><span class="p">,</span>  <span class="mf">2.25</span><span class="o">...</span><span class="p">,</span>  <span class="mf">1.84</span><span class="o">...</span><span class="p">])</span>
</code></pre></div>
<p>由于tf-idf经常用于文本特征，因此有另一个类称为<em>TfidfVectorizer</em>，将<em>CountVectorizer</em>和<em>TfidfTransformer</em>的所有选项合并在一个模型中：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="o">...</span>                                
<span class="o">&lt;</span><span class="mi">4</span><span class="n">x9</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">19</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>
</code></pre></div>
<p>尽管tf-idf的正态化也非常有用，在一些情况下，binary occurrence markers通常比特征更好。可以用<em>CountVectorizer</em>的<em>二元</em>参数达到这个目的。特别是，一些预测器比如<a href="http://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes">Bernoulli Naive Bayes</a>显性建模离散的布尔随机变量。非常短的文本也可能有满是噪音的tf-idf值，而binary occurrence info则更加稳定。</p>

<p>同样，调整特征抽取参数的最佳唯一方式是使用交叉验证网格搜索（grid search），例如，通过分类器用管道传输特征抽取器：
<a href="http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py">Sample pipeline for text feature extraction and evaluation</a></p>

<h2>解码文本文件</h2>

<p>文本由文字构成，但是文件由字节构成。在Unicode中，可能的文字会比可能的字节多很多。每一个文本文件都是编码的，因此，文字也可以用字节表示。</p>

<p>当你在python中处理文本时，应该都是Unicode。在scikit-learn中的大多数文本特征抽取器只能用于Unicode。因此，正确的从文件（或者从网络）加载文本，你需要用正确的编码解码。</p>

<p>编码也被称为&quot;字符集&quot;（“charset”或“character set”），尽管这个术语不准确。<em>CountVectorizer</em>用<em>encoding</em>参数告诉它用什么编码去解码。</p>

<p>对现代的文本文件来说，正确的编码可能是UTF-8。<em>CountVectorizer</em>用<em>encoding=&#39;utf-8&#39;</em>作为编码默认值。如果你加载的文档实际上不是UTF-8编码，那么你将获得<em>UnicodeDecodeError</em>错误。</p>

<p>如果你在解码文本时出现了问题，有一些东西可以尝试一下：</p>

<ul>
<li>找到文本的实际编码。文件可能包含一个文件头告诉你它的编码，或者根据文本的来自哪里你可以猜测一些标准的编码。</li>
<li>用UNIX命令<em>file</em>，你可以找到它是什么编码。Python的chardet模块有一个叫做<em>chardetect.py</em>的脚本，这个脚本会猜测具体的编码，尽管你不能期望它的猜测就是正确的。</li>
<li>你可以忽略错误，试一下UTF-8。你可以用<em>bytes.decode(errors=&#39;replace&#39;)</em>来解码字节字符串，用一个无意义的字符来替换所有解码错误，或者在向量器中设置<em>decode_error=&#39;replace&#39;</em>。这可能特征的有用性。</li>
<li>真实的文本可能来自多种来源使用多种编码，甚至可能凌乱的用错误的编码解码。在网络上的文本检索过程中这非常常见。Python的包<a href="https://github.com/LuminosoInsight/python-ftfy">ftfy</a>可以自动挑选出几类解码错误，因此，你可以尝试尝试将位置文档解码为<em>latin-1</em>，然后用<em>ftfy</em>来修复错误。</li>
<li>如果文本是一坨乱七八糟的东西（20个新闻组的数据集就是这样），很难简单的分类出编码，你可以回滚到单字符编码，比如latin-1。一些文本的显示可能不正确，但是，至少相同的字节序列都会代表相同的特征。</li>
</ul>

<h2>应用例子</h2>

<p>词袋表征非常简单，但是在实际应用中出奇的有用。</p>

<p>特别是在有监督的环境下，可以与快读可扩展的线性模型一起去训练文档分类器，例如：</p>

<ul>
<li><a href="http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html#example-document-classification-20newsgroups-py">Classification of text documents using sparse features</a></li>
</ul>

<p>在无监督的环境下，可以通过应用聚类算法比如<a href="http://scikit-learn.org/stable/modules/clustering.html#k-means">K-means</a>将相同的文档聚集成组：</p>

<ul>
<li><a href="Clustering%20text%20documents%20using%20k-means">Clustering text documents using k-means</a></li>
</ul>

<p>最后通过relaxing the hard assignment constraint of clustering可以发现语料库的主要主题，例如使用<a href="http://scikit-learn.org/stable/modules/decomposition.html#nmf">Non-negative matrix factorization (NMF or NNMF)</a>：</p>

<ul>
<li><a href="http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html#example-applications-topics-extraction-with-nmf-py">Topics extraction with Non-Negative Matrix Factorization</a></li>
</ul>

<h2>词袋表征的局限</h2>

<p>一组unigrams（即词袋）无法捕捉短语和多词（multi-word）表达，词库模型并不能解释可能的拼写错误或派生词。</p>

<p>N-grams来帮忙！与构建简单的一组unigrams相比，人们更倾向于构建一组bigrams（n=2），计数一组成对连续出现的词的频率。</p>

<p>人们也可能考虑一组字母的n-grams，可以处理错误拼写和派生词的表征。</p>

<p>例如，我们处理包含两个文档的语料库：[&#39;words&#39;, &#39;wprds&#39;]。第二个文档包含“words”这个词的错拼。简单的词袋表征会认为两个文档是完全不同的文档，两个可能的特征是不同的。但是，字母的n-grams表征可以发现两个文档匹配8个特征中的4个，这有助于分类器更好的决策：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s">&#39;words&#39;</span><span class="p">,</span> <span class="s">&#39;wprds&#39;</span><span class="p">])</span>
<span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39; w&#39;</span><span class="p">,</span> <span class="s">&#39;ds&#39;</span><span class="p">,</span> <span class="s">&#39;or&#39;</span><span class="p">,</span> <span class="s">&#39;pr&#39;</span><span class="p">,</span> <span class="s">&#39;rd&#39;</span><span class="p">,</span> <span class="s">&#39;s &#39;</span><span class="p">,</span> <span class="s">&#39;wo&#39;</span><span class="p">,</span> <span class="s">&#39;wp&#39;</span><span class="p">])</span>

<span class="bp">True</span>

<span class="n">counts</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</code></pre></div>
<p>在上面例子中，使用了“char_wb”分析器，这个分析器只从词边界（在两次填充空格）内部创建字母n-grams。“char”分析器则相反，跨词创建n-grams：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s">&#39;jumpy fox&#39;</span><span class="p">])</span>

<span class="o">&lt;</span><span class="mi">1</span><span class="n">x4</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">int64</span><span class="s">&#39;&gt;&#39;</span>
   <span class="k">with</span> <span class="mi">4</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Column</span> <span class="n">format</span><span class="o">&gt;</span>

<span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39; fox &#39;</span><span class="p">,</span> <span class="s">&#39; jump&#39;</span><span class="p">,</span> <span class="s">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s">&#39;umpy &#39;</span><span class="p">])</span>

<span class="bp">True</span>

<span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s">&#39;char&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s">&#39;jumpy fox&#39;</span><span class="p">])</span>

<span class="o">&lt;</span><span class="mi">1</span><span class="n">x5</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">int64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">5</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Column</span> <span class="n">format</span><span class="o">&gt;</span>

<span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
     <span class="p">[</span><span class="s">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s">&#39;mpy f&#39;</span><span class="p">,</span> <span class="s">&#39;py fo&#39;</span><span class="p">,</span> <span class="s">&#39;umpy &#39;</span><span class="p">,</span> <span class="s">&#39;y fox&#39;</span><span class="p">])</span>

<span class="bp">True</span>
</code></pre></div>
<p>区分边界的词变体char_wb对那些使用白空格进行词分隔的语言更加有效，因为在这些语言中，与那些原始字母变体相比，这种方式产生的特征噪音显著减少。对于这些语言，使用这些特征可以增加分类器预测的准确性和收敛的速度，同时保障了w.r.t.的错拼和派生词的强壮性。</p>

<p>尽管可以通过抽取n-grams而不是单独的词保留一部分局部位置信息，但是，词袋和bag of n-grams破坏了绝大多数文档的内部结构，以及内部结构所携带的大部分意义。</p>

<p>为了解决自然语言理解的更广泛任务，应该考虑句子和段落的局部结构。许多模型因此将被转换为“结构化输出”的问题，不过这些问题目前超出了scikit-learn的范围。</p>

<h2>用哈希技巧向量化大文本向量</h2>

<p>以上的向量化情景很简单，但是，事实上这种方式<em>从字符标记到整型特征的目录（vocabulary_属性）的映射都是在内存中进行</em>，在<em>处理大数据集</em>时会出现一些问题：
- 语料库越大，词表就会越大，因此使用的内存也越大，
- 拟合（fitting）需要根据原始数据集的大小等比例分配中间数据结构的大小。
- 构建词映射需要完整的传递数据集，因此不可能以严格在线的方式拟合文本分类器。
- pickling和un-pickling vocabulary<em>很大的向量器会非常慢（通常比pickling/un-pickling单纯数据的结构，比如同等大小的Numpy数组），
- 将向量化任务分隔成并行的子任务很不容易实现，因为vocabulary</em>属性要共享状态有一个细颗粒度的同步障碍：从标记字符串中映射特征索引与每个标记的首次出现顺序是独立的，因此应该被共享，在这点上并行worker的性能收到了损害，使他们比串行更慢。</p>

<p>通过同时使用由sklearn.feature_extraction.FeatureHasher类实施的“哈希技巧”（特征哈希）、文本预处理和CountVectorizer的标记特征有可能克服这些限制。</p>

<p>这个组合在HashingVectorizer实现，这个转换器类是无状态的，其大部分API与CountVectorizer.HashingVectorizer兼容，这意味着你不需要在上面调用fit：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>
<span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="o">...</span>                                
<span class="o">&lt;</span><span class="mi">4</span><span class="n">x10</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">16</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>
</code></pre></div>
<p>你可以看到从向量输出中抽取了16个非0特征标记：与之前由CountVectorizer在同一个样本语料库抽取的19个非0特征要少。差异来自哈希方法的冲突，因为较低的n_features参数的值。</p>

<p>在真实世界的环境下，n_features参数可以使用默认值2 ** 20（将近100万可能的特征）。如果内存或者下游模型的大小是一个问题，那么选择一个较小的值比如2 ** 18可能有一些帮助，而不需要为典型的文本分类任务引入太多额外的冲突。</p>

<p>注意维度并不影响CPU的算法训练时间，这部分是在操作CSR指标（LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive），但是，它对CSC matrices (LinearSVC(dual=False), Lasso(), etc)算法有效。</p>

<p>让我们用默认设置再试一下：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">()</span>
<span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="o">...</span>                               
<span class="o">&lt;</span><span class="mi">4</span><span class="n">x1048576</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s">&#39;&lt;... &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">19</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="n">format</span><span class="o">&gt;</span>
</code></pre></div>
<p>冲突没有再出现，但是，代价是输出空间的维度值非常大。当然，这里使用的19词以外的其他词之前仍会有冲突。</p>

<p>HashingVectorizer也有以下的局限：</p>

<ul>
<li>不能反转模型（没有inverse_transform方法），也无法访问原始的字符串表征，因为，进行mapping的哈希方法是单向本性。</li>
<li>没有提供了IDF权重，因为这需要在模型中引入状态。如果需要的话，可以在管道中添加TfidfTransformer。</li>
</ul>

<h2>进行HashingVectorizer的核外扩展</h2>

<p>使用HashingVectorizer的一个有趣发展是进行核外扩展的能力。这意味着我们可以从无法放入电脑主内存的数据中进行学习。</p>

<p>实现核外扩展的一个策略是将数据以流的方式以一小批提交给评估器。每批的向量化都是用HashingVectorizer这样来保证评估器的输入空间的维度是相等的。因此任何时间使用的内存数都限定在小频次的大小。尽管用这种方法可以处理的数据没有限制，但是从实用角度学习时间受到想要在这个任务上花费的CPU时间的限制。</p>

<p>一个核外扩展的文本分类任务的实例，请参见<a href="http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py">Out-of-core classification of text documents</a>.</p>

<p><a href="http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction">原文地址</a></p>

    </div>

    

    
      <ul class="tag_box inline">
        <li><i class="icon-tags"></i></li>
        
        


  
     
    	<li><a href="/tags.html#sklearn-ref">sklearn <span>1</span></a></li>
     
    	<li><a href="/tags.html#Python-ref">Python <span>17</span></a></li>
     
    	<li><a href="/tags.html#text feature extraction-ref">text feature extraction <span>1</span></a></li>
     
    	<li><a href="/tags.html#feature extraction-ref">feature extraction <span>1</span></a></li>
    
  



      </ul>
    

    <hr>
    
    <ul class="pagination">
      
        <li class="prev"><a href="/python/2014/01/12/PythonMultiprocessingintro" title="Python多进程模块Multiprocessing介绍">&larr; 上一页</a></li>
      
        <li><a href="/archive.html">归档列表</a></li>
      
        <li class="next"><a href="/python/2014/01/27/Gensim_Corpora_and_Vector_Spaces" title="gensim文档-语料库与向量空间">下一页 &rarr;</a></li>
      
    </ul>
    <hr>
    <!-- Paste the 3 next lines where you want the sharing button(s) to appear -->
    <div class="post-sharing">
     


  <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
	<span class="jiathis_txt">分享到：</span>
	<a class="jiathis_button_tools_1"></a>
	<a class="jiathis_button_tools_2"></a>
	<a class="jiathis_button_tools_3"></a>
	<a class="jiathis_button_tools_4"></a>
	<a href="http://www.jiathis.com/share?uid=1654363" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
	<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
var jiathis_config = {data_track_clickback:'true'};
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1343300786690926" charset="utf-8"></script>
<!-- JiaThis Button END -->



    </div>    
    <div class="post-comments">
    


  <!-- UJian Button BEGIN -->
<div class="ujian-hook"></div>
<script type="text/javascript">var ujian_config = {num:12,picSize:84,textHeight:45};</script>
<script type="text/javascript" src="http://v1.ujian.cc/code/ujian.js?uid=1654363"></script>
<a href="http://www.ujian.cc" style="border:0;"><img src="http://img.ujian.cc/pixel.png" alt="友荐云推荐" style="border:0;padding:0;margin:0;" /></a>
<!-- UJian Button END -->
<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=1654363"></script>
<!-- UY END -->



    </div>
  </div>
</div>


      <hr>
      <footer>
        <p>
          &copy; 2014 Cloga Chen
          <span class="pull-right text-muted">
            powered by
            <a href="http://jekyll-bootstrap-3.github.io" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll-Bootstrap-3</a>
            and <a href="http://getbootstrap.com" target="_blank">Twitter Bootstrap 3.0.3</a>
          </span>
        </p>
      </footer>
    </div>

    
    <script src="/assets/themes/bootstrap/resources/jquery/jquery.min.js"></script>
    <script src="/assets/themes/bootstrap/resources/bootstrap/js/bootstrap.min.js"></script>
<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-7VF4"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-7VF4');</script>
<!-- End Google Tag Manager -->


  </body>
</html>

